{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!---\n",
        "  Licensed to the Apache Software Foundation (ASF) under one\n",
        "  or more contributor license agreements.  See the NOTICE file\n",
        "  distributed with this work for additional information\n",
        "  regarding copyright ownership.  The ASF licenses this file\n",
        "  to you under the Apache License, Version 2.0 (the\n",
        "  \"License\"); you may not use this file except in compliance\n",
        "  with the License.  You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "  Unless required by applicable law or agreed to in writing,\n",
        "  software distributed under the License is distributed on an\n",
        "  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
        "  KIND, either express or implied.  See the License for the\n",
        "  specific language governing permissions and limitations\n",
        "  under the License.\n",
        "-->\n",
        "\n",
        "# Distributed Queries with Ballista\n",
        "\n",
        "This notebook demonstrates distributed query execution features in Ballista.\n",
        "\n",
        "## Overview\n",
        "\n",
        "Ballista is a distributed query engine that can execute queries across multiple\n",
        "nodes. When you submit a query, Ballista:\n",
        "\n",
        "1. Parses and optimizes the query\n",
        "2. Creates a distributed execution plan\n",
        "3. Distributes work across executors\n",
        "4. Collects and returns results\n",
        "\n",
        "This enables processing of datasets much larger than a single machine's memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ballista import BallistaSessionContext, setup_test_cluster\n",
        "from datafusion import col, lit\n",
        "from datafusion import functions as f\n",
        "\n",
        "# Set up test cluster and connect\n",
        "host, port = setup_test_cluster()\n",
        "ctx = BallistaSessionContext(f\"df://{host}:{port}\")\n",
        "\n",
        "# Register sample data\n",
        "ctx.register_parquet(\"test_data\", \"../testdata/test.parquet\")\n",
        "\n",
        "print(f\"Connected! Session ID: {ctx.session_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Execution Plans\n",
        "\n",
        "Understanding execution plans is key to optimizing distributed queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a query with multiple stages\n",
        "df = ctx.sql(\"\"\"\n",
        "    SELECT \n",
        "        bool_col,\n",
        "        COUNT(*) as cnt,\n",
        "        SUM(id) as sum_id,\n",
        "        AVG(tinyint_col) as avg_tiny\n",
        "    FROM test_data\n",
        "    WHERE id > 2\n",
        "    GROUP BY bool_col\n",
        "    ORDER BY cnt DESC\n",
        "\"\"\")\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View the logical plan\n",
        "print(\"Logical Plan:\")\n",
        "print(df.explain())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the execution plan\n",
        "# This shows the query plan as a graph (requires graphviz for full SVG)\n",
        "df.explain_visual()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View the plan with runtime statistics (analyze=True runs the query)\n",
        "print(\"Analyzed Plan (with statistics):\")\n",
        "print(df.explain(analyze=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multi-Stage Queries\n",
        "\n",
        "Complex queries may involve multiple stages of distributed execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Subquery example\n",
        "result = ctx.sql(\"\"\"\n",
        "    WITH stats AS (\n",
        "        SELECT \n",
        "            bool_col,\n",
        "            COUNT(*) as cnt\n",
        "        FROM test_data\n",
        "        GROUP BY bool_col\n",
        "    )\n",
        "    SELECT \n",
        "        t.id,\n",
        "        t.bool_col,\n",
        "        s.cnt as group_count\n",
        "    FROM test_data t\n",
        "    JOIN stats s ON t.bool_col = s.bool_col\n",
        "    WHERE t.id <= 5\n",
        "    ORDER BY t.id\n",
        "\"\"\")\n",
        "\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View the execution plan - notice the join and exchange stages\n",
        "result.explain_visual()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DataFrame API for Complex Transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build a complex transformation using the DataFrame API\n",
        "df1 = ctx.table(\"test_data\")\n",
        "\n",
        "# Aggregate to get group statistics\n",
        "group_stats = df1.aggregate(\n",
        "    [col(\"bool_col\")],\n",
        "    [\n",
        "        f.count_star().alias(\"group_count\"),\n",
        "        f.avg(col(\"id\")).alias(\"avg_id\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "group_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Join original data with statistics\n",
        "joined = df1.join(\n",
        "    group_stats,\n",
        "    on=\"bool_col\",\n",
        "    how=\"inner\"\n",
        ")\n",
        "\n",
        "joined.select(\"id\", \"bool_col\", \"group_count\", \"avg_id\").limit(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Window Functions\n",
        "\n",
        "Window functions allow computations across related rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Window function example\n",
        "window_result = ctx.sql(\"\"\"\n",
        "    SELECT \n",
        "        id,\n",
        "        bool_col,\n",
        "        tinyint_col,\n",
        "        SUM(tinyint_col) OVER (\n",
        "            PARTITION BY bool_col \n",
        "            ORDER BY id\n",
        "        ) as running_sum,\n",
        "        ROW_NUMBER() OVER (\n",
        "            PARTITION BY bool_col \n",
        "            ORDER BY id\n",
        "        ) as row_num\n",
        "    FROM test_data\n",
        "    ORDER BY bool_col, id\n",
        "\"\"\")\n",
        "\n",
        "window_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Progress Tracking for Long Queries\n",
        "\n",
        "For long-running queries, you can track progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute a query with progress tracking\n",
        "df = ctx.sql(\"SELECT * FROM test_data\")\n",
        "\n",
        "# collect_with_progress shows elapsed time in Jupyter\n",
        "batches = df.collect_with_progress()\n",
        "print(f\"Collected {len(batches)} batch(es)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# You can also provide a custom callback\n",
        "def my_progress_callback(status, progress):\n",
        "    if progress < 0:\n",
        "        print(f\"Status: {status} (in progress...)\")\n",
        "    else:\n",
        "        print(f\"Status: {status} ({progress:.0%} complete)\")\n",
        "\n",
        "df = ctx.sql(\"SELECT * FROM test_data\")\n",
        "batches = df.collect_with_progress(callback=my_progress_callback)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Writing Results\n",
        "\n",
        "Distributed write operations for large result sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare a query result\n",
        "df = ctx.sql(\"\"\"\n",
        "    SELECT \n",
        "        id,\n",
        "        bool_col,\n",
        "        tinyint_col * 2 as doubled\n",
        "    FROM test_data\n",
        "    WHERE id > 3\n",
        "\"\"\")\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write to Parquet (distributed write)\n",
        "# df.write_parquet(\"../target/output.parquet\")\n",
        "\n",
        "# Write to CSV\n",
        "# df.write_csv(\"../target/output.csv\")\n",
        "\n",
        "# Write to JSON\n",
        "# df.write_json(\"../target/output.json\")\n",
        "\n",
        "print(\"Write operations are commented out - uncomment to test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Practices for Distributed Queries\n",
        "\n",
        "1. **Filter early**: Push filters as close to the data source as possible\n",
        "2. **Project early**: Select only needed columns to reduce data movement\n",
        "3. **Partition wisely**: Ensure data is partitioned for efficient joins\n",
        "4. **Check plans**: Use `explain()` and `explain_visual()` to understand execution\n",
        "5. **Monitor progress**: Use `collect_with_progress()` for long queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Optimized query pattern\n",
        "optimized = (\n",
        "    ctx.table(\"test_data\")\n",
        "    # 1. Filter early\n",
        "    .filter(col(\"id\") > lit(2))\n",
        "    # 2. Project only needed columns\n",
        "    .select(\"id\", \"bool_col\", \"tinyint_col\")\n",
        "    # 3. Aggregate\n",
        "    .aggregate(\n",
        "        [col(\"bool_col\")],\n",
        "        [f.count_star().alias(\"cnt\")]\n",
        "    )\n",
        ")\n",
        "\n",
        "# 4. Check the plan\n",
        "print(\"Optimized plan:\")\n",
        "print(optimized.explain())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "- Review the [Ballista Architecture docs](https://datafusion.apache.org/ballista/)\n",
        "- Learn about cluster deployment and configuration\n",
        "- Explore advanced features like custom functions and plugins"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
