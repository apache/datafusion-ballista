/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 * <p>
 * http://www.apache.org/licenses/LICENSE-2.0
 * <p>
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

syntax = "proto3";

package ballista.protobuf;

option java_multiple_files = true;
option java_package = "org.apache.arrow.ballista.protobuf";
option java_outer_classname = "BallistaProto";

import "datafusion.proto";
import "datafusion_common.proto";

///////////////////////////////////////////////////////////////////////////////////////////////////
// Ballista Physical Plan
///////////////////////////////////////////////////////////////////////////////////////////////////
message BallistaPhysicalPlanNode {
  oneof PhysicalPlanType {
    ShuffleWriterExecNode shuffle_writer = 1;
    ShuffleReaderExecNode shuffle_reader = 2;
    UnresolvedShuffleExecNode unresolved_shuffle = 3;
  }
}

message ShuffleWriterExecNode {
  //TODO it seems redundant to provide job and stage id here since we also have them
  // in the TaskDefinition that wraps this plan
  string job_id = 1;
  uint32 stage_id = 2;
  datafusion.PhysicalPlanNode input = 3;
  datafusion.PhysicalHashRepartition output_partitioning = 4;
}

message UnresolvedShuffleExecNode {
  uint32 stage_id = 1;
  datafusion_common.Schema schema = 2;
  uint32 output_partition_count = 4;
}

message ShuffleReaderExecNode {
  repeated ShuffleReaderPartition partition = 1;
  datafusion_common.Schema schema = 2;
  // The stage to read from
  uint32 stage_id = 3;
}

message ShuffleReaderPartition {
  // each partition of a shuffle read can read data from multiple locations
  repeated PartitionLocation location = 1;
}

///////////////////////////////////////////////////////////////////////////////////////////////////
// Ballista Scheduling
///////////////////////////////////////////////////////////////////////////////////////////////////
message ExecutionGraph {
  string job_id = 1;
  string session_id = 2;
  JobStatus status = 3;
  repeated ExecutionGraphStage stages = 4;
  uint64 output_partitions = 5;
  repeated PartitionLocation output_locations = 6;
  string scheduler_id = 7;
  uint32 task_id_gen = 8;
  repeated StageAttempts failed_attempts = 9;
  string job_name = 10;
  uint64 start_time = 11;
  uint64 end_time = 12;
  uint64 queued_at = 13;
}

message StageAttempts {
  uint32 stage_id = 1;
  repeated uint32 stage_attempt_num = 2;
}

message ExecutionGraphStage {
  oneof StageType {
    UnResolvedStage unresolved_stage = 1;
    ResolvedStage resolved_stage = 2;
    SuccessfulStage successful_stage = 3;
    FailedStage failed_stage = 4;
  }
}

message UnResolvedStage {
  uint32 stage_id = 1;
  repeated uint32 output_links = 3;
  repeated  GraphStageInput inputs = 4;
  bytes plan = 5;
  uint32 stage_attempt_num = 6;
  repeated string last_attempt_failure_reasons = 7;
}

message ResolvedStage {
  uint32 stage_id = 1;
  uint32 partitions = 2;
  repeated uint32 output_links = 4;
  repeated  GraphStageInput inputs = 5;
  bytes plan = 6;
  uint32 stage_attempt_num = 7;
  repeated string last_attempt_failure_reasons = 8;
}

message SuccessfulStage {
  uint32 stage_id = 1;
  uint32 partitions = 2;
  repeated uint32 output_links = 4;
  repeated  GraphStageInput inputs = 5;
  bytes plan = 6;
  repeated TaskInfo task_infos = 7;
  repeated OperatorMetricsSet stage_metrics = 8;
  uint32 stage_attempt_num = 9;
}

message FailedStage {
  uint32 stage_id = 1;
  uint32 partitions = 2;
  repeated uint32 output_links = 4;
  bytes plan = 5;
  repeated TaskInfo task_infos = 6;
  repeated OperatorMetricsSet stage_metrics = 7;
  string error_message = 8;
  uint32 stage_attempt_num = 9;
}

message TaskInfo {
  uint32 task_id = 1;
  uint32 partition_id = 2;
  // Scheduler schedule time
  uint64 scheduled_time = 3;
  // Scheduler launch time
  uint64 launch_time = 4;
  // The time the Executor start to run the task
  uint64 start_exec_time = 5;
  // The time the Executor finish the task
  uint64 end_exec_time = 6;
  // Scheduler side finish time
  uint64 finish_time = 7;
  oneof status {
    RunningTask running = 8;
    FailedTask failed = 9;
    SuccessfulTask successful = 10;
  }
}

message GraphStageInput {
  uint32 stage_id = 1;
  repeated TaskInputPartitions partition_locations = 2;
  bool complete = 3;
}

message TaskInputPartitions {
  uint32 partition = 1;
  repeated PartitionLocation partition_location = 2;
}

message KeyValuePair {
  string key = 1;
  optional string value = 2;
}

message Action {

  oneof ActionType {
    // Fetch a partition from an executor
    FetchPartition fetch_partition = 3;
  }

  // configuration settings
  repeated KeyValuePair settings = 100;
}

message ExecutePartition {
  string job_id = 1;
  uint32 stage_id = 2;
  repeated uint32 partition_id = 3;
  datafusion.PhysicalPlanNode plan = 4;
  // The task could need to read partitions from other executors
  repeated PartitionLocation partition_location = 5;
  // Output partition for shuffle writer
  datafusion.PhysicalHashRepartition output_partitioning = 6;
}

message FetchPartition {
  string job_id = 1;
  uint32 stage_id = 2;
  uint32 partition_id = 3;
  string path = 4;
  string host = 5;
  uint32 port = 6;
}

message PartitionLocation {
  // partition_id of the map stage who produces the shuffle.
  uint32 map_partition_id = 1;
  // partition_id of the shuffle, a composition of(job_id + map_stage_id + partition_id).
  PartitionId partition_id = 2;
  ExecutorMetadata executor_meta = 3;
  PartitionStats partition_stats = 4;
  string path = 5;
}

// Unique identifier for a materialized partition of data
message PartitionId {
  string job_id = 1;
  uint32 stage_id = 2;
  uint32 partition_id = 4;
}

message TaskId {
  uint32 task_id = 1;
  uint32 task_attempt_num = 2;
  uint32 partition_id = 3;
}

message PartitionStats {
  int64 num_rows = 1;
  int64 num_batches = 2;
  int64 num_bytes = 3;
  repeated ColumnStats column_stats = 4;
}

message ColumnStats {
  datafusion_common.ScalarValue min_value = 1;
  datafusion_common.ScalarValue max_value = 2;
  uint32 null_count = 3;
  uint32 distinct_count = 4;
}

message OperatorMetricsSet {
  repeated OperatorMetric metrics = 1;
}


message NamedCount {
  string name = 1;
  uint64 value = 2;
}

message NamedGauge {
  string name = 1;
  uint64 value = 2;
}

message NamedTime {
  string name = 1;
  uint64 value = 2;
}

message OperatorMetric {
  oneof metric {
    uint64 output_rows = 1;
    uint64 elapse_time = 2;
    uint64 spill_count = 3;
    uint64 spilled_bytes = 4;
    uint64 current_memory_usage = 5;
    NamedCount count = 6;
    NamedGauge gauge = 7;
    NamedTime time = 8;
    int64 start_timestamp = 9;
    int64 end_timestamp = 10;
    uint64 spilled_rows = 11;
  }
}

// Used by scheduler
message ExecutorMetadata {
  string id = 1;
  string host = 2;
  uint32 port = 3;
  uint32 grpc_port = 4;
  ExecutorSpecification specification = 5;
}


// Used for scheduler-executor 
// communication
message ExecutorRegistration {
  string id = 1;
  optional string host = 2;
  uint32 port = 3;
  uint32 grpc_port = 4;
  ExecutorSpecification specification = 5;
}

message ExecutorHeartbeat {
  string executor_id = 1;
  // Unix epoch-based timestamp in seconds
  uint64 timestamp = 2;
  repeated ExecutorMetric metrics = 3;
  ExecutorStatus status = 4;
}

message ExecutorMetric {
  // TODO add more metrics
  oneof metric {
    uint64 available_memory = 1;
  }
}

message ExecutorStatus {
  oneof status {
    string active = 1;
    string dead = 2;
    string unknown = 3;
    string terminating = 4;
  }
}

message ExecutorSpecification {
  repeated ExecutorResource resources = 1;
}

message ExecutorResource {
  // TODO add more resources
  oneof resource {
    uint32 task_slots = 1;
  }
}

message AvailableTaskSlots {
  string executor_id = 1;
  uint32 slots = 2;
    }

message ExecutorTaskSlots {
  repeated AvailableTaskSlots task_slots = 1;
}

message ExecutorData {
  string executor_id = 1;
  repeated ExecutorResourcePair resources = 2;
}

message ExecutorResourcePair {
  ExecutorResource total = 1;
  ExecutorResource available = 2;
}

message RunningTask {
  string executor_id = 1;
}

message FailedTask {
  string error = 1;
  bool retryable = 2;
  // Whether this task failure should be counted to the maximum number of times the task is allowed to retry
  bool count_to_failures = 3;
  oneof failed_reason {
    ExecutionError execution_error = 4;
    FetchPartitionError fetch_partition_error = 5;
    IOError io_error = 6;
    ExecutorLost executor_lost = 7;
    // A successful task's result is lost due to executor lost
    ResultLost result_lost = 8;
    TaskKilled task_killed = 9;
  }
}

message SuccessfulTask {
  string executor_id = 1;
  // TODO tasks are currently always shuffle writes but this will not always be the case
  // so we might want to think about some refactoring of the task definitions
  repeated ShuffleWritePartition partitions = 2;
}

message ExecutionError {
}

message FetchPartitionError {
  string executor_id = 1;
  uint32 map_stage_id = 2;
  uint32 map_partition_id = 3;
}

message IOError {
}

message ExecutorLost {
}

message ResultLost {
}

message TaskKilled {
}

message ShuffleWritePartition {
  uint64 partition_id = 1;
  string path = 2;
  uint64 num_batches = 3;
  uint64 num_rows = 4;
  uint64 num_bytes = 5;
}

message TaskStatus {
  uint32 task_id = 1;
  string job_id = 2;
  uint32 stage_id = 3;
  uint32 stage_attempt_num = 4;
  uint32 partition_id = 5;
  uint64 launch_time = 6;
  uint64 start_exec_time = 7;
  uint64 end_exec_time = 8;
  oneof status {
    RunningTask running = 9;
    FailedTask failed = 10;
    SuccessfulTask successful = 11;
  }
  repeated OperatorMetricsSet metrics = 12;
}

message PollWorkParams {
  ExecutorRegistration metadata = 1;
  uint32 num_free_slots = 2;
  // All tasks must be reported until they reach the failed or completed state
  repeated TaskStatus task_status = 3;
}

message TaskDefinition {
  uint32 task_id = 1;
  uint32 task_attempt_num = 2;
  string job_id = 3;
  uint32 stage_id = 4;
  uint32 stage_attempt_num = 5;
  uint32 partition_id = 6;
  bytes plan = 7;
  string session_id = 9;
  uint64 launch_time = 10;
  repeated KeyValuePair props = 11;
}

// A set of tasks in the same stage
message MultiTaskDefinition {
  repeated TaskId task_ids = 1;
  string job_id = 2;
  uint32 stage_id = 3;
  uint32 stage_attempt_num = 4;
  bytes plan = 5;
  string session_id = 7;
  uint64 launch_time = 8;
  repeated KeyValuePair props = 9;
}

message JobSessionConfig {
  string session_id = 1;
  repeated KeyValuePair configs = 2;
}

message PollWorkResult {
  repeated TaskDefinition tasks = 1;
}

message RegisterExecutorParams {
  ExecutorRegistration metadata = 1;
}

message RegisterExecutorResult {
  bool success = 1;
}

message HeartBeatParams {
  string executor_id = 1;
  repeated ExecutorMetric metrics = 2;
  ExecutorStatus status = 3;
  ExecutorRegistration metadata = 4;
}

message HeartBeatResult {
  // TODO it's from Spark for BlockManager
  bool reregister = 1;
}

message StopExecutorParams {
  string executor_id = 1;
  // stop reason
  string reason = 2;
  // force to stop the executor immediately
  bool force = 3;
}

message StopExecutorResult {
}

message ExecutorStoppedParams {
  string executor_id = 1;
  // stop reason
  string reason = 2;
}

message ExecutorStoppedResult {
}

message UpdateTaskStatusParams {
  string executor_id = 1;
  // All tasks must be reported until they reach the failed or completed state
  repeated TaskStatus task_status = 2;
}

message UpdateTaskStatusResult {
  bool success = 1;
}

message ExecuteQueryParams {
  oneof query {
    bytes logical_plan = 1;
    string sql = 2 [deprecated=true]; // I'd suggest to remove this, if SQL needed use `flight-sql`
  }
  
  optional string session_id = 3;
  repeated KeyValuePair settings = 4;
}

message CreateSessionParams {
  repeated KeyValuePair settings = 1;
}

message CreateSessionResult {
  string session_id = 1;
}

message UpdateSessionParams {
  string session_id = 1;
  repeated KeyValuePair settings = 2;
}

message UpdateSessionResult {
  bool success = 1;
}

message RemoveSessionParams {
  string session_id = 1;
}

message RemoveSessionResult {
  bool success = 1;
}

message ExecuteSqlParams {
  string sql = 1;
}

message ExecuteQueryResult {
  oneof result {
    ExecuteQuerySuccessResult success = 1;
    ExecuteQueryFailureResult failure = 2;
  }
}

message ExecuteQuerySuccessResult {
  string job_id = 1;
  string session_id = 2;
}

message ExecuteQueryFailureResult {
  oneof failure {
    string session_not_found = 1;
    string plan_parsing_failure = 2;
    string sql_parsing_failure = 3;
  }
}

message GetJobStatusParams {
  string job_id = 1;
}

message SuccessfulJob {
  repeated PartitionLocation partition_location = 1;
  uint64 queued_at = 2;
  uint64 started_at = 3;
  uint64 ended_at = 4;
}

message QueuedJob {
  uint64 queued_at = 1;
}

// TODO: add progress report
message RunningJob {
  uint64 queued_at = 1;
  uint64 started_at = 2;
  string scheduler = 3;
}

message FailedJob {
  string error = 1;
  uint64 queued_at = 2;
  uint64 started_at = 3;
  uint64 ended_at = 4;
}

message JobStatus {
  string job_id = 5;
  string job_name = 6;

  oneof status {
    QueuedJob queued = 1;
    RunningJob running = 2;
    FailedJob failed = 3;
    SuccessfulJob successful = 4;
  }
}

message GetJobStatusResult {
  JobStatus status = 1;
}

message FilePartitionMetadata {
  repeated string filename = 1;
}

message CancelJobParams {
  string job_id = 1;
}

message CancelJobResult {
  bool cancelled = 1;
}

message CleanJobDataParams {
  string job_id = 1;
}

message CleanJobDataResult {
}

message LaunchTaskParams {
  // Allow to launch a task set to an executor at once
  repeated TaskDefinition tasks = 1;
  string scheduler_id = 2;
}

message LaunchMultiTaskParams {
  // Allow to launch a task set to an executor at once
  repeated MultiTaskDefinition multi_tasks = 1;
  string scheduler_id = 2;
}

message LaunchTaskResult {
  bool success = 1;
  // TODO when part of the task set are scheduled successfully
}

message LaunchMultiTaskResult {
  bool success = 1;
  // TODO when part of the task set are scheduled successfully
}

message CancelTasksParams {
  repeated RunningTaskInfo task_infos = 1;
}

message CancelTasksResult {
  bool cancelled = 1;
}

message RemoveJobDataParams {
  string job_id = 1;
}

message RemoveJobDataResult {
}

message RunningTaskInfo {
  uint32 task_id = 1;
  string job_id = 2;
  uint32 stage_id = 3;
  uint32 partition_id = 4;;
}

service SchedulerGrpc {
  // Executors must poll the scheduler for heartbeat and to receive tasks
  rpc PollWork (PollWorkParams) returns (PollWorkResult) {}

  rpc RegisterExecutor(RegisterExecutorParams) returns (RegisterExecutorResult) {}

  // Push-based task scheduler will only leverage this interface
  // rather than the PollWork interface to report executor states
  rpc HeartBeatFromExecutor (HeartBeatParams) returns (HeartBeatResult) {}

  rpc UpdateTaskStatus (UpdateTaskStatusParams) returns (UpdateTaskStatusResult) {}

  rpc CreateSession (CreateSessionParams) returns (CreateSessionResult) {}

  rpc UpdateSession (UpdateSessionParams) returns (UpdateSessionResult) {}

  rpc RemoveSession (RemoveSessionParams) returns (RemoveSessionResult) {}

  rpc ExecuteQuery (ExecuteQueryParams) returns (ExecuteQueryResult) {}

  rpc GetJobStatus (GetJobStatusParams) returns (GetJobStatusResult) {}

  // Used by Executor to tell Scheduler it is stopped.
  rpc ExecutorStopped (ExecutorStoppedParams) returns (ExecutorStoppedResult) {}

  rpc CancelJob (CancelJobParams) returns (CancelJobResult) {}

  rpc CleanJobData (CleanJobDataParams) returns (CleanJobDataResult) {}
}

service ExecutorGrpc {
  rpc LaunchTask (LaunchTaskParams) returns (LaunchTaskResult) {}

  rpc LaunchMultiTask (LaunchMultiTaskParams) returns (LaunchMultiTaskResult) {}

  rpc StopExecutor (StopExecutorParams) returns (StopExecutorResult) {}

  rpc CancelTasks (CancelTasksParams) returns (CancelTasksResult) {}

  rpc RemoveJobData (RemoveJobDataParams) returns (RemoveJobDataResult) {}
}
